Oracle's AI Got Smart :
You've probably seen the headlines: enterprise databases now have AI. Oracle's DBMS_CLOUD_AI.GENERATE() function lets you write natural language queries and get SQL back. Here assumption is you have done basic setup for Oracle 26ai on Oracle Autonomous Database hosted on Oracle Cloud Infrastructure (OCI).

I always feel that SQL always seems to be a prompt languange even before the advent of AI and further I got more interested on how it behaves under new development and the new version.

I tested this systematically on the TPC-H benchmark (22 real-world queries across three complexity levels) and found that AI generates valid SQL syntax. But in a database, "running without errors" isn't the same as "being right."

In my baseline testing, Oracle’s AI achieved 100% syntactic success—the queries were valid SQL—but only a 63.64% semantic match rate. This means that in nearly 4 out of 10 cases, the AI gave me a "successful" query that returned the wrong data. For enterprise decision-making, that’s a dangerous failure rate.

Why AI SQL Generation Fails And Is There A way To Improve?
Let me show you the three patterns that break AI SQL generation:

Pattern 1: Semantic Ambiguity
User: "Find orders for Customer#1"
AI generates: SELECT * FROM ORDERS WHERE C_CUSTKEY = 1
Error: Column "C_CUSTKEY" doesn't exist in ORDERS table
Expected: SELECT * FROM ORDERS WHERE O_CUSTKEY = 1
The AI knows about customers and orders but doesn't understand that "Customer#1" maps to different column names in different tables. Obvious to a human. Confusing to an LLM trained on generic SQL.

Pattern 2: Formula Comprehension Gaps
User: "Total revenue accounting for discounts"
AI generates: SELECT SUM(EXTENDEDPRICE - DISCOUNT) FROM LINEITEM
Error: Wrong values (arithmetic is wrong)
Expected: SELECT SUM(EXTENDEDPRICE * (1 - DISCOUNT)) FROM LINEITEM
The model knows discounts exist but doesn't understand the actual calculation semantics. It's like asking someone who's heard of compound interest but never used it to calculate it correctly.

Pattern 3: Complex Pattern Chains
User: "Top 5 customers by total spending"
AI generates: SELECT C_CUSTKEY FROM CUSTOMER ... (incomplete/wrong structure)
Error: Missing JOIN, GROUP BY, or ORDER BY
Expected: Multi-part query with JOIN, aggregation, and FETCH FIRST
When queries require combining multiple SQL patterns (JOIN + GROUP BY + ORDER BY + FETCH), the AI struggles to orchestrate them correctly.

In my testing, 36% of queries hit one of these three problems. But here's the key insight: all three are directly addressable through better prompting.

Prompt Engineering Solution
I tested three approaches to see what actually works:

Baseline (Generated By Oracle 26ai)
prompt = "Show top 5 most expensive orders"
result = DBMS_CLOUD_AI.GENERATE(prompt, action='showsql')
# Returns incorrect or partial SQL
# Success rate: 64% (14/22 queries)
Unsurprisingly, generic natural language doesn't work well with enterprise schemas without any context in my research.

Enhanced Strategy (Schema Context + Domain Hints)
prompt = """
Database Schema:
- ORDERS table: O_ORDERKEY, O_CUSTKEY, O_TOTALPRICE, O_ORDERDATE
- LINEITEM table: L_ORDERKEY, L_EXTENDEDPRICE, L_DISCOUNT, L_QUANTITY
- CUSTOMER table: C_CUSTKEY, C_NAME, C_NATIONKEY

ENTITY NAMING CONVENTIONS:
- Customer references like "Customer#1" use ID columns (C_CUSTKEY = 1)
- For discount calculations: multiply EXTENDEDPRICE * (1 - DISCOUNT)
- ORDER BY ... FETCH FIRST X ROWS ONLY for top N queries (Oracle syntax)

GENERATION GUIDELINES:
1. Always verify table and column names against schema above
2. Use FETCH FIRST X ROWS ONLY for pagination (not LIMIT)
3. Join relationships follow star schema (fact-to-dimension)

Question: Show top 5 most expensive orders
"""
result = DBMS_CLOUD_AI.GENERATE(prompt, action='showsql')
# Returns correct SQL
# Success rate: 86% (19/22 queries)
Improvement: +22 percentage points, achieved through simple documentation.

Make it learn through (Schema + Working Examples)
prompt = """
[Schema as above]

WORKING EXAMPLES:
Example 1 - Discount Calculation:
  Q: "What is total revenue accounting for discounts?"
  A: SELECT SUM(L_EXTENDEDPRICE * (1 - L_DISCOUNT)) FROM LINEITEM

Example 2 - Entity Reference:
  Q: "Show orders for Customer#1"
  A: SELECT * FROM ORDERS WHERE O_CUSTKEY = 1

Example 3 - Top N with Sorting:
  Q: "Top 5 expensive orders?"
  A: SELECT * FROM ORDERS ORDER BY O_TOTALPRICE DESC FETCH FIRST 5 ROWS ONLY

Question: Show top 5 most expensive orders
"""
result = DBMS_CLOUD_AI.GENERATE(prompt, action='showsql')
# Returns correct SQL (same accuracy as Enhanced)
# Success rate: 86% (19/22 queries)
Surprising finding: Adding example queries didn't actually help. Just giving the AI a clear list of your tables and columns (schema documentation) was enough to fix most problems. Examples were nice to have, but they weren't the real bottleneck-good documentation was.

Improvements:
Here's the accuracy by query complexity :

COMPLEXITY	BASELINE	ENHANCED	IMPROVEMENT
Simple	75%	75%	—
Medium	50%	75%	+25%
Complex	70%	100%	+30%
Overall	64%	86%	+22%
What's interesting:

Simple queries were already working (schema context didn't help much)
Medium queries got the biggest boost (+25%) - these are where domain context matters most
Complex queries jumped from 70% to 100% (5 previously-failed queries now pass)
Latency: LLM Thinking Vs Oracle Execution 
Here's what surprised me: LLM generation time dominates everything.

Average breakdown:
- LLM thinking time: 3,300 ms (≈ 3.3 seconds)
- Oracle execution: 47 ms (≈ 0.047 seconds)
- Overhead ratio: 69.9x (LLM dominates completely)
This is important for deployment decisions - batching queries or accepting 3-4 second latency is necessary unless you switch to a faster LLM backend.

Note that prompt length doesn't hurt latency. Adding schema context and examples adds ~300 tokens (negligible overhead).

Three queries couldn't be fixed by prompting alone:

QUERY	ISSUE	ROOT CAUSE
Q6	Top 5 orders (wrong syntax)	ROWNUM pattern unfamiliar to model
Q10	Customer entity mapping	Context-dependent disambiguation too hard
Q14	Part filtering	Likely missing training data
Assessment: These failures appear to be model training limitations, not prompt engineering failures. They'd require fine-tuning or a different model entirely.

How to Implement This in Your System
Rather than paste incomplete code snippets here, I've published a complete, production-ready implementation on GitHub that you can clone and customize immediately.

Get the Full Implementation
GitHub Repository: https://github.com/sanjay/oracle26ai-eval

The repo includes everything you need:

Schema context builder - Generate prompts from your database schema automatically
Domain hints for multiple industries - Pre-built templates for finance, retail, healthcare
Evaluation script - Test any query set (your own or the 22 TPC-H queries)
All data & results - CSV files showing baseline vs enhanced accuracy
Production deployment guide - Step-by-step instructions to integrate into your app
Quick Start (5 Minutes)
# Clone the repository
git clone https://github.com/sanmish4ds/oracle26ai-eval.git

# Test on your own queries
python main.py --queries your_queries.csv --database oracle
What's Inside
main.py - Entry point; runs baseline and enhanced evaluation
db_utils.py - Oracle database connection and query execution
enhanced_strategy.py - Ready-to-use schema context + domain hints implementation
test_enhanced_strategy_all_queries.py - Full 22-query benchmark (you can adapt this)
CSV files with results - See real before/after numbers
Customize for Your Database
The code is database-agnostic. Just update:

Your database schema (auto-extracted from INFORMATION_SCHEMA)
Your domain-specific rules (e.g., how discounts are calculated in your business)
Your query log (replace TPC-H with your actual queries)
Limitation:

Single Database System: Only tested on Oracle. PostgreSQL and MySQL might have different results.
Fixed Dataset: TPC-H is a standard benchmark, but your production queries might have different failure patterns.
No Fine-tuning: I deliberately avoided model fine-tuning to show prompt engineering in isolation. Fine-tuning might add another 5-10%.
Summary:
AI SQL generation is production-ready today, but only if you engineer the prompts. Schema context and domain hints can take you from "technically working but unusable" (64%) to "pragmatically viable" (86%) without model changes or infrastructure investment.

References:
1. Zhong, V., et al. (2017). "Seq2SQL: Generating Structured Queries from Natural Language." arXiv.

2. Yu, T., et al. (2018). "Spider: A Large-Scale Human-Labeled Dataset for Complex and Cross-Domain Semantic Parsing and Text-to-SQL Task." EMNLP.

3. OpenAI (2023). "GPT-4 Technical Report." arXiv.

4. Oracle (2024). "Oracle AI SQL Generation Documentation." Oracle Database Documentation.